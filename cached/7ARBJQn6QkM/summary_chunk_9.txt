mprove the share price or whatever it is. But you have to believe in your future. You have to 
invest in yourself. And we believe this so deeply that we invested you know tens 
of billions of dollars before it really happened. And yeah it was, it was 10 long 
years. But it was fun along the way. How would you summarize those core beliefs? What 
is it that you believe about the way computers should work and what they can do for us that keeps 
you not only coming through that decade but also doing what you're doing now, making bets I'm sure 
you're making for the next few decades? The first core belief was our first discussion, was about 
accelerated computing. Parallel computing versus general purpose computing. We would add 
two of those processors together and we would do accelerated computing. And I continue to believe 
that today. The second was the recognition that these deep learning networks, these DNNs, that 
came to the public during 2012, these deep neural networks have the ability to learn patterns and 
relationships from a whole bunch of different types of data. And that it can learn more and 
more nuanced features if it could be larger and larger. And it's easier to make them larger and 
larger, make them deeper and deeper or wider and wider, and so the scalability of the architecture 
is empirically true. The fact that model size and the data size being larger 
and larger, you can learn more knowledge is also true, empirically true. And so if that's 
the case, you could you know, what what are the limits? There not, unless there's a physical limit 
or an architectural limit or mathematical limit and it was never found, and so we believe that you 
could scale it. Then the question, the only other question is: What can you learn from data? What 
can you learn from experience? Data is basically digital versions of human experience. And so what 
can you learn? You obviously can learn object recognition from images. You can learn speech 
from just listening 
-> summary ->
*   A fundamental belief centers on accelerated computing, specifically the parallel processing of data to achieve enhanced computational outcomes.
*   The recognition of deep neural networks (DNNs) and their capacity for learning complex patterns through scalable architectures, driven by increasing model and data size, remains a core tenet.
*   Data is considered the primary source of knowledge, functioning as a digital representation of human experience, capable of generating substantial learning opportunities.
*   Scaling computational resources and data volume is seen as the key determinant of learning potential, with no inherent limitations anticipated beyond physical, architectural, or mathematical constraints.